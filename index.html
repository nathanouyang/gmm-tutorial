<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gaussian Mixture Models Tutorial</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="#">GMM Tutorial</a>
        </div>
    </nav>

    <div class="container mt-4">
        <h1 class="text-center mb-4">Model-based Clustering: Gaussian Mixture Models</h1>
        
        <section id="introduction" class="my-5">
            <h2>üìå Introduction</h2>
            <div class="row">
                <div class="col-md-8">
                    <h3>What is clustering?</h3>
                    <p>Clustering is an unsupervised learning technique that groups similar data points together based on certain features or characteristics. The goal is to find natural groupings in data without prior labels.</p>
                    
                    <h3>Limitations of hard clustering like K-Means</h3>
                    <p>Traditional clustering algorithms like K-Means assign each data point to exactly one cluster (hard clustering). This has several limitations:</p>
                    <ul>
                        <li>Assumes spherical clusters of similar sizes</li>
                        <li>Cannot express uncertainty in cluster assignments</li>
                        <li>Performs poorly with overlapping clusters</li>
                        <li>Sensitive to initialization and outliers</li>
                    </ul>
                    
                    <h3>Why GMM?</h3>
                    <p>Gaussian Mixture Models (GMMs) offer several advantages:</p>
                    <ul>
                        <li><strong>Soft clustering</strong>: Assigns probabilities of belonging to each cluster</li>
                        <li><strong>Flexible cluster shapes</strong>: Models elliptical clusters with different orientations and sizes</li>
                        <li><strong>Probabilistic framework</strong>: Provides uncertainty measures in clustering</li>
                        <li><strong>Principled approach</strong>: Based on a statistical model with clear assumptions</li>
                    </ul>
                    
                    <h3>Theory behind GMM</h3>
                    <p>A Gaussian Mixture Model is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions with unknown parameters. Mathematically, GMM represents the probability density of the data as a weighted sum of K Gaussian components, where each component has its own mean vector (Œº<sub>k</sub>) and covariance matrix (Œ£<sub>k</sub>). The model is defined by these parameters plus the mixture weights (œÄ<sub>k</sub>) that determine the relative contribution of each component. GMMs are typically trained using the Expectation-Maximization (EM) algorithm, which iteratively estimates the model parameters by alternating between assigning data points to components (E-step) and updating the parameters based on these assignments (M-step). This approach guarantees convergence to a local optimum of the likelihood function, making GMM a theoretically sound method for modeling complex data distributions.</p>
                    
                    <h3>GMM vs. K-means: A Deeper Comparison</h3>
                    <p>While both K-means and GMM are clustering algorithms, they differ fundamentally in their approach. K-means can be viewed as a simplified special case of GMM with equal, spherical covariance matrices and hard assignments. The key theoretical difference is that K-means minimizes within-cluster sum-of-squares distances, while GMM maximizes the likelihood of the data under a generative model. This difference has significant practical implications: K-means draws straight-line boundaries between clusters, which works well for spherical, well-separated clusters but fails for complex shapes. GMM, in contrast, learns the full covariance structure of each cluster, allowing it to capture elliptical shapes, varying cluster sizes, and different orientations. Additionally, GMM's probabilistic nature enables it to handle overlapping clusters gracefully by assigning probabilities instead of making hard decisions. This makes GMM more flexible but also more computationally intensive and potentially prone to overfitting with limited data. The choice between these algorithms ultimately depends on the specific characteristics of your data and the goals of your analysis.</p>
                </div>
                <div class="col-md-4">
                    <div class="card">
                        <div class="card-body">
                            <h5 class="card-title">Key Concept</h5>
                            <p>A Gaussian Mixture Model represents a distribution as a weighted sum of Gaussian distributions.</p>
                            <div id="gmm-intro-formula" class="formula-display"></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="algorithm-walkthrough" class="my-5">
            <h2>üî¨ Step-by-Step: How GMM Works</h2>
            <p class="lead">Let's dive deep into the inner workings of the Gaussian Mixture Model algorithm by walking through each step of the process.</p>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>The Algorithm Behind GMM</h3>
                    <p>The Gaussian Mixture Model uses the Expectation-Maximization (EM) algorithm to find the maximum likelihood parameters of the mixture model. This iterative algorithm allows us to find the parameters of a statistical model where there are unobserved latent variables - in this case, the cluster assignments.</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>1. Model Definition</h3>
                    <p>A Gaussian Mixture Model represents a probability distribution as a weighted sum of K Gaussian components:</p>
                    
                    <div class="card mb-4">
                        <div class="card-body">
                            <div class="formula-display">
                                p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
                            </div>
                        </div>
                    </div>
                    
                    <p>Where:</p>
                    <ul>
                        <li><strong>p(x)</strong> is the probability density at point x</li>
                        <li><strong>K</strong> is the number of mixture components (clusters)</li>
                        <li><strong>œÄ<sub>k</sub></strong> are the mixture weights (must sum to 1)</li>
                        <li><strong>N(x|Œº<sub>k</sub>, Œ£<sub>k</sub>)</strong> is the probability density of the multivariate Gaussian distribution with mean Œº<sub>k</sub> and covariance matrix Œ£<sub>k</sub></li>
                    </ul>
                    
                    <p>The model parameters we need to estimate are:</p>
                    <ul>
                        <li>Mixture weights: œÄ<sub>1</sub>, œÄ<sub>2</sub>, ..., œÄ<sub>K</sub></li>
                        <li>Means: Œº<sub>1</sub>, Œº<sub>2</sub>, ..., Œº<sub>K</sub></li>
                        <li>Covariance matrices: Œ£<sub>1</sub>, Œ£<sub>2</sub>, ..., Œ£<sub>K</sub></li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <h3>2. Latent Variables and Log-Likelihood</h3>
                    <p>For each data point x<sub>i</sub>, we introduce a latent variable z<sub>i</sub> representing which component generated that point. We can think of z<sub>i</sub> as a K-dimensional one-hot vector where z<sub>ik</sub> = 1 if point i belongs to component k, and 0 otherwise.</p>
                    
                    <p>The complete-data log-likelihood (if we knew the latent variables) would be:</p>
                    
                    <div class="card mb-4">
                        <div class="card-body">
                            <div class="formula-display">
                                \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{i=1}^{N} \sum_{k=1}^{K} z_{ik} \ln(\pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))
                            </div>
                        </div>
                    </div>
                    
                    <p>However, we don't know the true assignments, so we use the EM algorithm to iteratively optimize this likelihood.</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>3. Initialization</h3>
                    <p>We begin by initializing the parameters of our model. Proper initialization is crucial as the EM algorithm can converge to local optima. Common initialization strategies include:</p>
                    <ul>
                        <li><strong>Random initialization:</strong> Randomly select K data points as initial means, initialize covariances as identity matrices, and set weights to 1/K.</li>
                        <li><strong>K-means initialization:</strong> Run K-means clustering first and use the resulting centroids as initial means.</li>
                        <li><strong>Multiple restarts:</strong> Run the algorithm multiple times with different initializations and select the best result.</li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <div class="plot-container" id="em-process-plot"></div>
                    <p class="text-center">Evolution of GMM parameters over iterations</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>4. Expectation-Maximization (EM) Algorithm</h3>
                    <p>The EM algorithm alternates between two steps:</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <div class="card mb-4">
                        <div class="card-body">
                            <h4>E-Step (Expectation)</h4>
                            <p>In the E-step, we estimate the expected values of the latent variables given our current model parameters. This results in computing "responsibilities" - the posterior probability that component k generated data point i:</p>
                            <div class="formula-display">
                                \gamma(z_{ik}) = \frac{\pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
                            </div>
                            <p>These responsibilities replace the unknown binary indicators z<sub>ik</sub> with their expected values. Each data point now has a probability distribution over all K clusters instead of a single cluster assignment.</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card mb-4">
                        <div class="card-body">
                            <h4>M-Step (Maximization)</h4>
                            <p>In the M-step, we update our model parameters to maximize the expected complete-data log-likelihood, using the responsibilities from the E-step:</p>
                            <div class="formula-display">
                                \boldsymbol{\mu}_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) \mathbf{x}_i}{\sum_{i=1}^{N} \gamma(z_{ik})}
                            </div>
                            <div class="formula-display">
                                \boldsymbol{\Sigma}_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) (\mathbf{x}_i - \boldsymbol{\mu}_k^{new})(\mathbf{x}_i - \boldsymbol{\mu}_k^{new})^T}{\sum_{i=1}^{N} \gamma(z_{ik})}
                            </div>
                            <div class="formula-display">
                                \pi_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik})}{N}
                            </div>
                            <p>We can interpret N<sub>k</sub> = Œ£<sub>i</sub> Œ≥(z<sub>ik</sub>) as the effective number of points assigned to cluster k.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-7">
                    <h3>5. Convergence</h3>
                    <p>After each E-step and M-step iteration, we repeat the process until the algorithm converges. Convergence can be determined by:</p>
                    <ul>
                        <li>Monitoring the change in log-likelihood between iterations</li>
                        <li>Checking if the parameter updates fall below some threshold</li>
                        <li>Reaching a maximum number of iterations</li>
                    </ul>
                    <p>The log-likelihood is guaranteed to increase (or remain the same) with each iteration, ensuring eventual convergence. However, it may converge to a local maximum rather than the global maximum.</p>
                    <p>The log-likelihood can be computed as:</p>
                    <div class="formula-display">
                        \ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{i=1}^{N} \ln \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)
                    </div>
                    
                    <p>Once the algorithm converges, we have our final model parameters - the mixture weights œÄ<sub>k</sub>, mean vectors Œº<sub>k</sub>, and covariance matrices Œ£<sub>k</sub> for each of the K components. These parameters completely define our Gaussian Mixture Model and can be used for clustering, density estimation, or generating new samples from the learned distribution.</p>
                </div>
                <div class="col-md-5">
                    <div class="plot-container" id="likelihood-plot"></div>
                    <p class="text-center">Log-likelihood increases with each iteration until convergence</p>
                </div>
            </div>
        </section>
        
        <section id="dataset-preparation" class="my-5">
            <h2>üìä Dataset Preparation</h2>
            <p>Let's explore GMMs using both synthetic data and real-world examples.</p>
            <div class="row">
                <div class="col-md-6">
                    <h3>Synthetic Dataset</h3>
                    <p>We'll generate a dataset with overlapping clusters to demonstrate the power of GMM.</p>
                    <div id="synthetic-data-plot" class="plot-container"></div>
                </div>
                <div class="col-md-6">
                    <h3>Code Example: Generating Synthetic Data</h3>
                    <pre class="code-block"><code class="python">
import numpy as np
from sklearn.datasets import make_blobs

# Generate synthetic data with 3 clusters
X, y_true = make_blobs(
    n_samples=300, 
    centers=3, 
    cluster_std=[1.0, 2.0, 0.5], 
    random_state=42
)

# Transform the data to make it more elliptical
transformation = np.array([[0.6, -0.6], [0.4, 0.8]])
X = np.dot(X, transformation)
                    </code></pre>
                </div>
            </div>
        </section>
        
        <section id="gaussians-refresher" class="my-5">
            <h2>üìò A Quick Refresher on Gaussians</h2>
            <div class="row">
                <div class="col-md-7">
                    <h3>The Multivariate Gaussian Distribution</h3>
                    <p>The multivariate Gaussian (or normal) distribution is a generalization of the one-dimensional normal distribution to higher dimensions. It is defined by:</p>
                    <ul>
                        <li><strong>Mean vector (Œº)</strong>: The center of the distribution</li>
                        <li><strong>Covariance matrix (Œ£)</strong>: Describes the shape, size, and orientation of the distribution</li>
                    </ul>
                    <p>The probability density function (PDF) of a d-dimensional multivariate Gaussian is:</p>
                    <div id="gaussian-pdf-formula" class="formula-display my-3"></div>
                    <p>where:</p>
                    <ul>
                        <li>x is a d-dimensional vector</li>
                        <li>Œº is the mean vector</li>
                        <li>Œ£ is the covariance matrix</li>
                        <li>|Œ£| is the determinant of Œ£</li>
                    </ul>
                </div>
                <div class="col-md-5">
                    <div id="gaussian-2d-plot" class="plot-container"></div>
                    <p class="text-center">2D Gaussian distribution contour plot</p>
                </div>
            </div>
        </section>
        
        <section id="kmeans-to-gmm" class="my-5">
            <h2>üîç From K-Means to GMM</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>K-Means vs GMM Comparison</h3>
                    <table class="table table-bordered">
                        <thead>
                            <tr>
                                <th>Feature</th>
                                <th>K-Means</th>
                                <th>GMM</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Assignment</td>
                                <td>Hard (binary)</td>
                                <td>Soft (probabilistic)</td>
                            </tr>
                            <tr>
                                <td>Cluster shape</td>
                                <td>Spherical</td>
                                <td>Elliptical</td>
                            </tr>
                            <tr>
                                <td>Distance measure</td>
                                <td>Euclidean</td>
                                <td>Mahalanobis</td>
                            </tr>
                            <tr>
                                <td>Model complexity</td>
                                <td>Low</td>
                                <td>Higher</td>
                            </tr>
                            <tr>
                                <td>Training method</td>
                                <td>Iterative optimization</td>
                                <td>Expectation-Maximization (EM)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="col-md-6">
                    <h3>Visual Comparison</h3>
                    <div class="row">
                        <div class="col-md-6">
                            <div id="kmeans-plot" class="plot-container"></div>
                            <p class="text-center">K-Means clustering result</p>
                        </div>
                        <div class="col-md-6">
                            <div id="gmm-plot" class="plot-container"></div>
                            <p class="text-center">GMM clustering result</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="gmm-sklearn" class="my-5">
            <h2>üìà GMM with Scikit-Learn</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>Implementing GMM</h3>
                    <p>Scikit-learn provides a simple API for fitting Gaussian Mixture Models:</p>
                    <pre class="code-block"><code class="python">
from sklearn.mixture import GaussianMixture

# Create and fit the GMM model
gmm = GaussianMixture(
    n_components=3,    # Number of clusters
    covariance_type='full',  # Full covariance matrix
    random_state=42
)

# Fit the model to the data
gmm.fit(X)

# Get cluster assignments
labels = gmm.predict(X)

# Get probability of belonging to each cluster
probabilities = gmm.predict_proba(X)
                    </code></pre>
                </div>
                <div class="col-md-6">
                    <h3>Key Parameters</h3>
                    <ul>
                        <li><strong>n_components</strong>: Number of mixture components (clusters)</li>
                        <li><strong>covariance_type</strong>:
                            <ul>
                                <li>'full': Each component has its own general covariance matrix</li>
                                <li>'tied': All components share the same general covariance matrix</li>
                                <li>'diag': Each component has its own diagonal covariance matrix</li>
                                <li>'spherical': Each component has its own single variance</li>
                            </ul>
                        </li>
                        <li><strong>max_iter</strong>: Maximum number of EM iterations</li>
                        <li><strong>init_params</strong>: Method used to initialize the weights, means and precisions</li>
                    </ul>
                    <div id="gmm-result-plot" class="plot-container"></div>
                </div>
            </div>
        </section>
        
        <section id="soft-clustering" class="my-5">
            <h2>üß™ Soft Clustering</h2>
            <div class="row">
                <div class="col-md-7">
                    <h3>Visualizing Cluster Probabilities</h3>
                    <p>One of the key advantages of GMM is soft assignment - each point has a probability of belonging to each cluster.</p>
                    <pre class="code-block"><code class="python">
# Getting probability values
probabilities = gmm.predict_proba(X)

# Example: Visualizing point with uncertain assignment
point_idx = 42  # Some example point
print(f"Point {point_idx} probabilities:")
for i, prob in enumerate(probabilities[point_idx]):
    print(f"  Cluster {i}: {prob:.3f}")
                    </code></pre>
                    <p>This allows us to identify points that are on the boundaries between clusters, where the assignment is uncertain.</p>
                </div>
                <div class="col-md-5">
                    <div id="soft-clustering-plot" class="plot-container"></div>
                    <p class="text-center">Visualization of cluster probabilities (darker colors indicate higher probability)</p>
                </div>
            </div>
        </section>
        
        <section id="hyperparameter-tuning" class="my-5">
            <h2>üõ† Hyperparameter Tuning</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>Choosing the Number of Components</h3>
                    <p>A critical parameter in GMM is the number of components. We can use information criteria to select this:</p>
                    <ul>
                        <li><strong>Akaike Information Criterion (AIC)</strong>: Penalizes model complexity</li>
                        <li><strong>Bayesian Information Criterion (BIC)</strong>: Similar to AIC but with stronger penalty for complexity</li>
                    </ul>
                    <div id="aic-bic-formula" class="formula-display"></div>
                </div>
                <div class="col-md-6">
                    <h3>Code Example: Model Selection</h3>
                    <pre class="code-block"><code class="python">
# Evaluate different numbers of components
n_components_range = range(1, 10)
models = [GaussianMixture(n_components=n, 
                          covariance_type='full', 
                          random_state=42).fit(X)
          for n in n_components_range]

aic = [model.aic(X) for model in models]
bic = [model.bic(X) for model in models]

# The best number of components is the one with lowest AIC/BIC
best_aic = n_components_range[np.argmin(aic)]
best_bic = n_components_range[np.argmin(bic)]
                    </code></pre>
                    <div id="aic-bic-plot" class="plot-container"></div>
                </div>
            </div>
        </section>
    </div>

    <footer class="bg-dark text-light py-3 mt-5">
        <div class="container text-center">
            <p>¬© 2023 GMM Tutorial | Created for educational purposes</p>
        </div>
    </footer>

    <!-- JavaScript dependencies -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.8.2/dist/d3.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/plotly.js@2.18.2/dist/plotly.min.js"></script>
    <script src="js/script.js"></script>
</body>
</html> 