<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gaussian Mixture Models Tutorial</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="#">GMM Tutorial</a>
        </div>
    </nav>

    <div class="container mt-4">
        <h1 class="text-center mb-4">Model-based Clustering: Gaussian Mixture Models</h1>
        
        <section id="introduction" class="my-5">
            <h2>📌 Introduction</h2>
            <div class="row">
                <div class="col-md-8">
                    <h3>What is clustering?</h3>
                    <p>Clustering is an unsupervised learning technique that groups similar data points together based on certain features or characteristics. The goal is to find natural groupings in data without prior labels.</p>
                    
                    <h3>Limitations of hard clustering like K-Means</h3>
                    <p>Traditional clustering algorithms like K-Means assign each data point to exactly one cluster (hard clustering). This has several limitations:</p>
                    <ul>
                        <li>Assumes spherical clusters of similar sizes</li>
                        <li>Cannot express uncertainty in cluster assignments</li>
                        <li>Performs poorly with overlapping clusters</li>
                        <li>Sensitive to initialization and outliers</li>
                    </ul>
                    
                    <h3>Why GMM?</h3>
                    <p>Gaussian Mixture Models (GMMs) offer several advantages:</p>
                    <ul>
                        <li><strong>Soft clustering</strong>: Assigns probabilities of belonging to each cluster</li>
                        <li><strong>Flexible cluster shapes</strong>: Models elliptical clusters with different orientations and sizes</li>
                        <li><strong>Probabilistic framework</strong>: Provides uncertainty measures in clustering</li>
                        <li><strong>Principled approach</strong>: Based on a statistical model with clear assumptions</li>
                    </ul>
                    
                    <h3>Theory behind GMM</h3>
                    <p>A Gaussian Mixture Model is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions with unknown parameters. Mathematically, GMM represents the probability density of the data as a weighted sum of K Gaussian components, where each component has its own mean vector (μ<sub>k</sub>) and covariance matrix (Σ<sub>k</sub>). The model is defined by these parameters plus the mixture weights (π<sub>k</sub>) that determine the relative contribution of each component. GMMs are typically trained using the Expectation-Maximization (EM) algorithm, which iteratively estimates the model parameters by alternating between assigning data points to components (E-step) and updating the parameters based on these assignments (M-step). This approach guarantees convergence to a local optimum of the likelihood function, making GMM a theoretically sound method for modeling complex data distributions.</p>
                    
                    <h3>GMM vs. K-means: A Deeper Comparison</h3>
                    <p>While both K-means and GMM are clustering algorithms, they differ fundamentally in their approach. K-means can be viewed as a simplified special case of GMM with equal, spherical covariance matrices and hard assignments. The key theoretical difference is that K-means minimizes within-cluster sum-of-squares distances, while GMM maximizes the likelihood of the data under a generative model. This difference has significant practical implications: K-means draws straight-line boundaries between clusters, which works well for spherical, well-separated clusters but fails for complex shapes. GMM, in contrast, learns the full covariance structure of each cluster, allowing it to capture elliptical shapes, varying cluster sizes, and different orientations. Additionally, GMM's probabilistic nature enables it to handle overlapping clusters gracefully by assigning probabilities instead of making hard decisions. This makes GMM more flexible but also more computationally intensive and potentially prone to overfitting with limited data. The choice between these algorithms ultimately depends on the specific characteristics of your data and the goals of your analysis.</p>
                </div>
                <div class="col-md-4">
                    <div class="card">
                        <div class="card-body">
                            <h5 class="card-title">Key Concept</h5>
                            <p>A Gaussian Mixture Model represents a distribution as a weighted sum of Gaussian distributions.</p>
                            <div id="gmm-intro-formula" class="formula-display"></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="algorithm-walkthrough" class="my-5">
            <h2>🔬 Step-by-Step: How GMM Works</h2>
            <p class="lead">Let's dive deep into the inner workings of the Gaussian Mixture Model algorithm by walking through each step of the process.</p>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>The Algorithm Behind GMM</h3>
                    <p>The Gaussian Mixture Model uses the Expectation-Maximization (EM) algorithm to find the maximum likelihood parameters of the mixture model. Below we'll explore exactly how this iterative process works, with both mathematical explanations and code implementation.</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>1. Initialization</h3>
                    <p>We start by initializing the parameters of our model:</p>
                    <ul>
                        <li><strong>Means (μ<sub>k</sub>)</strong>: Initial values for each cluster center</li>
                        <li><strong>Covariances (Σ<sub>k</sub>)</strong>: Initial covariance matrices for each cluster</li>
                        <li><strong>Mixture weights (π<sub>k</sub>)</strong>: Initial probabilities for each cluster</li>
                    </ul>
                    <p>Initialization is crucial as the EM algorithm can converge to local optima. Common strategies include:</p>
                    <ul>
                        <li>Random initialization (randomly choose data points as initial means)</li>
                        <li>K-means initialization (use K-means to find initial cluster centers)</li>
                        <li>Multiple restarts with different initializations</li>
                    </ul>
                    <pre class="code-block"><code class="python">
# Initialization with K-means (scikit-learn's default)
import numpy as np
from sklearn.cluster import KMeans

# Generate simple 2D data for illustration
np.random.seed(42)
# Create data with 3 clusters
X1 = np.random.multivariate_normal([-2, -2], [[1, 0], [0, 1]], 100)
X2 = np.random.multivariate_normal([0, 2], [[1, 0.5], [0.5, 1]], 100)
X3 = np.random.multivariate_normal([3, 0], [[1, -0.7], [-0.7, 1]], 100)
X = np.vstack([X1, X2, X3])

# Initialize with K-means
n_components = 3
kmeans = KMeans(n_clusters=n_components, n_init=10, random_state=42)
kmeans.fit(X)
initial_means = kmeans.cluster_centers_

# Initialize covariances as identity matrices
initial_covs = [np.eye(X.shape[1]) for _ in range(n_components)]

# Initialize weights uniformly
initial_weights = np.ones(n_components) / n_components

print("Initial means:\n", initial_means)
print("Initial weights:", initial_weights)
                    </code></pre>
                </div>
                <div class="col-md-6">
                    <h3>2. The EM Algorithm Iterations</h3>
                    <p>The EM algorithm alternates between two steps:</p>
                    <div class="card mb-4">
                        <div class="card-body">
                            <h4>E-Step (Expectation)</h4>
                            <p>Compute the "responsibilities" - the probability that each data point belongs to each cluster, given the current parameter estimates.</p>
                            <p>For each data point x<sub>i</sub> and each cluster k, the responsibility is:</p>
                            <div class="formula-display">
                                γ(z<sub>ik</sub>) = p(z<sub>k</sub>|x<sub>i</sub>, θ) = 
                                \frac{π<sub>k</sub> \mathcal{N}(x<sub>i</sub>|μ<sub>k</sub>, Σ<sub>k</sub>)}
                                {\sum_{j=1}^{K} π<sub>j</sub> \mathcal{N}(x<sub>i</sub>|μ<sub>j</sub>, Σ<sub>j</sub>)}
                            </div>
                            <p>Where z<sub>k</sub> is the kth cluster assignment, and θ represents all parameters.</p>
                        </div>
                    </div>
                    <div class="card">
                        <div class="card-body">
                            <h4>M-Step (Maximization)</h4>
                            <p>Update the parameters using the current responsibilities:</p>
                            <ul>
                                <li><strong>Update means:</strong> μ<sub>k</sub> = (1/N<sub>k</sub>) Σ<sub>i</sub> γ(z<sub>ik</sub>)x<sub>i</sub></li>
                                <li><strong>Update covariances:</strong> Σ<sub>k</sub> = (1/N<sub>k</sub>) Σ<sub>i</sub> γ(z<sub>ik</sub>)(x<sub>i</sub>-μ<sub>k</sub>)(x<sub>i</sub>-μ<sub>k</sub>)<sup>T</sup></li>
                                <li><strong>Update weights:</strong> π<sub>k</sub> = N<sub>k</sub>/N</li>
                            </ul>
                            <p>Where N<sub>k</sub> = Σ<sub>i</sub> γ(z<sub>ik</sub>) is the effective number of points assigned to cluster k.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>3. Python Implementation of the EM Steps</h3>
                    <p>Let's see how the EM algorithm would be implemented in Python:</p>
                    <pre class="code-block"><code class="python">
def gaussian_pdf(x, mean, cov):
    """Evaluate multivariate Gaussian PDF at point x"""
    n = mean.shape[0]
    det = np.linalg.det(cov)
    inv = np.linalg.inv(cov)
    norm_const = 1.0 / (np.power(2 * np.pi, n/2) * np.sqrt(det))
    x_mean = x - mean
    result = norm_const * np.exp(-0.5 * x_mean.T @ inv @ x_mean)
    return result

def em_algorithm(X, initial_means, initial_covs, initial_weights, max_iter=100, tol=1e-6):
    """Custom implementation of EM for GMM"""
    n_samples, n_features = X.shape
    n_components = len(initial_means)
    
    # Initialize parameters
    means = initial_means.copy()
    covs = initial_covs.copy()
    weights = initial_weights.copy()
    
    # Initialize log-likelihood
    log_likelihood = 0
    
    # Visualization data
    all_means = [means.copy()]
    all_covs = [covs.copy()]
    all_weights = [weights.copy()]
    all_log_likelihood = []
    
    # EM iterations
    for iteration in range(max_iter):
        # Store previous log-likelihood
        prev_log_likelihood = log_likelihood
        
        #------------------------
        # E-step: Calculate responsibilities
        #------------------------
        responsibilities = np.zeros((n_samples, n_components))
        
        for i in range(n_samples):
            # Calculate probability of x_i under each Gaussian
            pdf_values = np.array([
                weights[k] * gaussian_pdf(X[i], means[k], covs[k])
                for k in range(n_components)
            ])
            
            # Normalize to get responsibilities
            responsibilities[i] = pdf_values / np.sum(pdf_values)
        
        #------------------------
        # M-step: Update parameters
        #------------------------
        # Effective number of points in each cluster
        N_k = np.sum(responsibilities, axis=0)
        
        # Update means
        new_means = np.zeros_like(means)
        for k in range(n_components):
            # Weighted sum of data points
            new_means[k] = np.sum(responsibilities[:, k, np.newaxis] * X, axis=0) / N_k[k]
        means = new_means
        
        # Update covariances
        new_covs = []
        for k in range(n_components):
            diff = X - means[k]
            # Weighted covariance calculation
            cov_k = np.zeros((n_features, n_features))
            for i in range(n_samples):
                x_diff = diff[i].reshape(-1, 1)
                cov_k += responsibilities[i, k] * (x_diff @ x_diff.T)
            cov_k /= N_k[k]
            
            # Add small regularization to ensure positive definiteness
            cov_k += 1e-6 * np.eye(n_features)
            new_covs.append(cov_k)
        covs = new_covs
        
        # Update weights
        weights = N_k / n_samples
        
        # Calculate log-likelihood
        log_likelihood = 0
        for i in range(n_samples):
            log_likelihood += np.log(sum([
                weights[k] * gaussian_pdf(X[i], means[k], covs[k])
                for k in range(n_components)
            ]))
        
        # Store current parameters
        all_means.append(means.copy())
        all_covs.append(covs.copy())
        all_weights.append(weights.copy())
        all_log_likelihood.append(log_likelihood)
        
        # Check for convergence
        if abs(log_likelihood - prev_log_likelihood) < tol:
            break
    
    return means, covs, weights, all_means, all_covs, all_weights, all_log_likelihood

# Run our EM algorithm on the data
means, covs, weights, all_means, all_covs, all_weights, all_log_likelihood = em_algorithm(
    X, initial_means, initial_covs, initial_weights
)

print(f"Final means after EM:\n{means}")
print(f"Final weights: {weights}")
                    </code></pre>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>4. Visualizing the EM Process</h3>
                    <p>A key insight into the EM algorithm is seeing how it evolves over iterations. At each step:</p>
                    <ul>
                        <li>Cluster means shift toward dense regions of data</li>
                        <li>Covariance matrices adapt to the shape of clusters</li>
                        <li>Mixture weights adjust based on the proportion of points assigned to each cluster</li>
                        <li>Log-likelihood increases until convergence</li>
                    </ul>
                    <p>The visualization on the right shows how the algorithm progresses from initialization to convergence, demonstrating the EM algorithm's ability to identify complex cluster structures.</p>
                </div>
                <div class="col-md-6">
                    <div class="plot-container" id="em-process-plot"></div>
                    <p class="text-center">Evolution of GMM parameters over iterations</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-7">
                    <h3>5. Convergence</h3>
                    <p>The EM algorithm is guaranteed to increase the log-likelihood at each iteration. The algorithm typically converges when:</p>
                    <ul>
                        <li>The change in log-likelihood between iterations falls below a threshold, or</li>
                        <li>The maximum number of iterations is reached.</li>
                    </ul>
                    <p>However, it's important to note that EM only guarantees convergence to a <em>local</em> maximum of the likelihood function, not necessarily the global maximum. This is why initialization is crucial, and multiple runs with different starting points are often used.</p>
                    <pre class="code-block"><code class="python">
# Plot log-likelihood over iterations to visualize convergence
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(all_log_likelihood, 'o-')
plt.title('Log-Likelihood vs Iteration')
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.tight_layout()
plt.show()
                    </code></pre>
                </div>
                <div class="col-md-5">
                    <div class="plot-container" id="likelihood-plot"></div>
                    <p class="text-center">Log-likelihood increases with each iteration until convergence</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>6. Comparison with scikit-learn's Implementation</h3>
                    <p>Let's compare our manual implementation with scikit-learn's optimized implementation:</p>
                    <pre class="code-block"><code class="python">
from sklearn.mixture import GaussianMixture

# Initialize GMM with same parameters as our implementation
gmm = GaussianMixture(
    n_components=3,
    means_init=initial_means,
    weights_init=initial_weights,
    precisions_init=[np.linalg.inv(cov) for cov in initial_covs],
    random_state=42
)

# Fit the model
gmm.fit(X)

# Print results
print("Scikit-learn means:\n", gmm.means_)
print("Scikit-learn weights:", gmm.weights_)

# Predict cluster assignments
labels = gmm.predict(X)

# Visualize results
plt.figure(figsize=(10, 8))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)
    
# Plot the means and covariance ellipses
from matplotlib.patches import Ellipse

def plot_ellipse(pos, cov, ax=None, **kwargs):
    """Plot an ellipse based on a covariance matrix."""
    ax = ax or plt.gca()
    
    # Calculate eigenvalues and eigenvectors
    vals, vecs = np.linalg.eigh(cov)
    order = vals.argsort()[::-1]
    vals = vals[order]
    vecs = vecs[:, order]
    
    # Width and height of ellipse
    width, height = 2 * np.sqrt(vals)
    
    # Rotation in degrees
    theta = np.degrees(np.arctan2(vecs[1, 0], vecs[0, 0]))
    
    # Draw the ellipse
    ellipse = Ellipse(
        xy=pos, width=width, height=height,
        angle=theta, **kwargs
    )
    ax.add_patch(ellipse)
    return ellipse

plt.figure(figsize=(10, 8))

# Plot the data points
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)

# Plot the means and covariance ellipses
for i, (mean, covar) in enumerate(zip(gmm.means_, gmm.covariances_)):
    plot_ellipse(
        mean, covar, 
        edgecolor='red', 
        facecolor='none', 
        linewidth=2, 
        alpha=0.8
    )
    plt.scatter(mean[0], mean[1], s=100, c='red', marker='x')

plt.xlim(-6, 6)
plt.ylim(-6, 6)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('GMM Clustering Results')
plt.tight_layout()
plt.show()
                    </code></pre>
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-6">
                    <h3>7. Practical Considerations</h3>
                    <ul>
                        <li><strong>Initialization Strategies:</strong> K-means initialization often works well, but multiple restarts can help avoid poor local optima.</li>
                        <li><strong>Covariance Regularization:</strong> Adding a small constant to the diagonal of covariance matrices ensures numerical stability.</li>
                        <li><strong>Handling Degenerate Cases:</strong> When a cluster has very few points, its covariance can become ill-conditioned.</li>
                        <li><strong>Computational Complexity:</strong> The EM algorithm scales as O(NKD²) where N is the number of samples, K is the number of components, and D is the dimensionality of the data.</li>
                        <li><strong>Memory Requirements:</strong> Storing responsibilities for all points across all clusters requires O(NK) memory.</li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <h3>8. Advanced Variations</h3>
                    <ul>
                        <li><strong>Constrained Covariance Types:</strong>
                            <ul>
                                <li>'full': Each component has its own unrestricted covariance matrix</li>
                                <li>'tied': All components share the same covariance matrix</li>
                                <li>'diag': Each component has its own diagonal covariance matrix</li>
                                <li>'spherical': Each component has its own single variance parameter</li>
                            </ul>
                        </li>
                        <li><strong>Variational Bayesian GMM:</strong> Automatically determines the effective number of components by using a Dirichlet process prior.</li>
                        <li><strong>Stochastic EM:</strong> For large datasets, updates can be performed on mini-batches for computational efficiency.</li>
                        <li><strong>GMM with Missing Data:</strong> The EM algorithm can be extended to handle incomplete data.</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <section id="dataset-preparation" class="my-5">
            <h2>📊 Dataset Preparation</h2>
            <p>Let's explore GMMs using both synthetic data and real-world examples.</p>
            <div class="row">
                <div class="col-md-6">
                    <h3>Synthetic Dataset</h3>
                    <p>We'll generate a dataset with overlapping clusters to demonstrate the power of GMM.</p>
                    <div id="synthetic-data-plot" class="plot-container"></div>
                </div>
                <div class="col-md-6">
                    <h3>Code Example: Generating Synthetic Data</h3>
                    <pre class="code-block"><code class="python">
import numpy as np
from sklearn.datasets import make_blobs

# Generate synthetic data with 3 clusters
X, y_true = make_blobs(
    n_samples=300, 
    centers=3, 
    cluster_std=[1.0, 2.0, 0.5], 
    random_state=42
)

# Transform the data to make it more elliptical
transformation = np.array([[0.6, -0.6], [0.4, 0.8]])
X = np.dot(X, transformation)
                    </code></pre>
                </div>
            </div>
        </section>
        
        <section id="gaussians-refresher" class="my-5">
            <h2>📘 A Quick Refresher on Gaussians</h2>
            <div class="row">
                <div class="col-md-7">
                    <h3>The Multivariate Gaussian Distribution</h3>
                    <p>The multivariate Gaussian (or normal) distribution is a generalization of the one-dimensional normal distribution to higher dimensions. It is defined by:</p>
                    <ul>
                        <li><strong>Mean vector (μ)</strong>: The center of the distribution</li>
                        <li><strong>Covariance matrix (Σ)</strong>: Describes the shape, size, and orientation of the distribution</li>
                    </ul>
                    <p>The probability density function (PDF) of a d-dimensional multivariate Gaussian is:</p>
                    <div id="gaussian-pdf-formula" class="formula-display my-3"></div>
                    <p>where:</p>
                    <ul>
                        <li>x is a d-dimensional vector</li>
                        <li>μ is the mean vector</li>
                        <li>Σ is the covariance matrix</li>
                        <li>|Σ| is the determinant of Σ</li>
                    </ul>
                </div>
                <div class="col-md-5">
                    <div id="gaussian-2d-plot" class="plot-container"></div>
                    <p class="text-center">2D Gaussian distribution contour plot</p>
                </div>
            </div>
        </section>
        
        <section id="kmeans-to-gmm" class="my-5">
            <h2>🔍 From K-Means to GMM</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>K-Means vs GMM Comparison</h3>
                    <table class="table table-bordered">
                        <thead>
                            <tr>
                                <th>Feature</th>
                                <th>K-Means</th>
                                <th>GMM</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Assignment</td>
                                <td>Hard (binary)</td>
                                <td>Soft (probabilistic)</td>
                            </tr>
                            <tr>
                                <td>Cluster shape</td>
                                <td>Spherical</td>
                                <td>Elliptical</td>
                            </tr>
                            <tr>
                                <td>Distance measure</td>
                                <td>Euclidean</td>
                                <td>Mahalanobis</td>
                            </tr>
                            <tr>
                                <td>Model complexity</td>
                                <td>Low</td>
                                <td>Higher</td>
                            </tr>
                            <tr>
                                <td>Training method</td>
                                <td>Iterative optimization</td>
                                <td>Expectation-Maximization (EM)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="col-md-6">
                    <h3>Visual Comparison</h3>
                    <div class="row">
                        <div class="col-md-6">
                            <div id="kmeans-plot" class="plot-container"></div>
                            <p class="text-center">K-Means clustering result</p>
                        </div>
                        <div class="col-md-6">
                            <div id="gmm-plot" class="plot-container"></div>
                            <p class="text-center">GMM clustering result</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="gmm-sklearn" class="my-5">
            <h2>📈 GMM with Scikit-Learn</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>Implementing GMM</h3>
                    <p>Scikit-learn provides a simple API for fitting Gaussian Mixture Models:</p>
                    <pre class="code-block"><code class="python">
from sklearn.mixture import GaussianMixture

# Create and fit the GMM model
gmm = GaussianMixture(
    n_components=3,    # Number of clusters
    covariance_type='full',  # Full covariance matrix
    random_state=42
)

# Fit the model to the data
gmm.fit(X)

# Get cluster assignments
labels = gmm.predict(X)

# Get probability of belonging to each cluster
probabilities = gmm.predict_proba(X)
                    </code></pre>
                </div>
                <div class="col-md-6">
                    <h3>Key Parameters</h3>
                    <ul>
                        <li><strong>n_components</strong>: Number of mixture components (clusters)</li>
                        <li><strong>covariance_type</strong>:
                            <ul>
                                <li>'full': Each component has its own general covariance matrix</li>
                                <li>'tied': All components share the same general covariance matrix</li>
                                <li>'diag': Each component has its own diagonal covariance matrix</li>
                                <li>'spherical': Each component has its own single variance</li>
                            </ul>
                        </li>
                        <li><strong>max_iter</strong>: Maximum number of EM iterations</li>
                        <li><strong>init_params</strong>: Method used to initialize the weights, means and precisions</li>
                    </ul>
                    <div id="gmm-result-plot" class="plot-container"></div>
                </div>
            </div>
        </section>
        
        <section id="soft-clustering" class="my-5">
            <h2>🧪 Soft Clustering</h2>
            <div class="row">
                <div class="col-md-7">
                    <h3>Visualizing Cluster Probabilities</h3>
                    <p>One of the key advantages of GMM is soft assignment - each point has a probability of belonging to each cluster.</p>
                    <pre class="code-block"><code class="python">
# Getting probability values
probabilities = gmm.predict_proba(X)

# Example: Visualizing point with uncertain assignment
point_idx = 42  # Some example point
print(f"Point {point_idx} probabilities:")
for i, prob in enumerate(probabilities[point_idx]):
    print(f"  Cluster {i}: {prob:.3f}")
                    </code></pre>
                    <p>This allows us to identify points that are on the boundaries between clusters, where the assignment is uncertain.</p>
                </div>
                <div class="col-md-5">
                    <div id="soft-clustering-plot" class="plot-container"></div>
                    <p class="text-center">Visualization of cluster probabilities (darker colors indicate higher probability)</p>
                </div>
            </div>
        </section>
        
        <section id="hyperparameter-tuning" class="my-5">
            <h2>🛠 Hyperparameter Tuning</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>Choosing the Number of Components</h3>
                    <p>A critical parameter in GMM is the number of components. We can use information criteria to select this:</p>
                    <ul>
                        <li><strong>Akaike Information Criterion (AIC)</strong>: Penalizes model complexity</li>
                        <li><strong>Bayesian Information Criterion (BIC)</strong>: Similar to AIC but with stronger penalty for complexity</li>
                    </ul>
                    <div id="aic-bic-formula" class="formula-display"></div>
                </div>
                <div class="col-md-6">
                    <h3>Code Example: Model Selection</h3>
                    <pre class="code-block"><code class="python">
# Evaluate different numbers of components
n_components_range = range(1, 10)
models = [GaussianMixture(n_components=n, 
                          covariance_type='full', 
                          random_state=42).fit(X)
          for n in n_components_range]

aic = [model.aic(X) for model in models]
bic = [model.bic(X) for model in models]

# The best number of components is the one with lowest AIC/BIC
best_aic = n_components_range[np.argmin(aic)]
best_bic = n_components_range[np.argmin(bic)]
                    </code></pre>
                    <div id="aic-bic-plot" class="plot-container"></div>
                </div>
            </div>
        </section>
    </div>

    <footer class="bg-dark text-light py-3 mt-5">
        <div class="container text-center">
            <p>© 2023 GMM Tutorial | Created for educational purposes</p>
        </div>
    </footer>

    <!-- JavaScript dependencies -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.8.2/dist/d3.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/plotly.js@2.18.2/dist/plotly.min.js"></script>
    <script src="js/script.js"></script>
</body>
</html> 