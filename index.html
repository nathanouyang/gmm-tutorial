<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gaussian Mixture Models Tutorial</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="#">GMM Tutorial</a>
        </div>
    </nav>

    <div class="container mt-4">
        <h1 class="text-center mb-4">Model-based Clustering: Gaussian Mixture Models</h1>
        
        <section id="introduction" class="my-5">
            <h2>üìå Introduction</h2>
            <div class="row">
                <div class="col-md-8">
                    <h3>What is clustering?</h3>
                    <p>Clustering is an unsupervised learning technique that groups similar data points together based on certain features or characteristics. The goal is to find natural groupings in data without prior labels.</p>
                    
                    <h3>Limitations of hard clustering like K-Means</h3>
                    <p>Traditional clustering algorithms like K-Means assign each data point to exactly one cluster (hard clustering). This has several limitations:</p>
                    <ul>
                        <li>Assumes spherical clusters of similar sizes</li>
                        <li>Cannot express uncertainty in cluster assignments</li>
                        <li>Performs poorly with overlapping clusters</li>
                        <li>Sensitive to initialization and outliers</li>
                    </ul>
                    
                    <h3>Why GMM?</h3>
                    <p>Gaussian Mixture Models (GMMs) offer several advantages:</p>
                    <ul>
                        <li><strong>Soft clustering</strong>: Assigns probabilities of belonging to each cluster</li>
                        <li><strong>Flexible cluster shapes</strong>: Models elliptical clusters with different orientations and sizes</li>
                        <li><strong>Probabilistic framework</strong>: Provides uncertainty measures in clustering</li>
                        <li><strong>Principled approach</strong>: Based on a statistical model with clear assumptions</li>
                    </ul>
                    
                    <h3>Theory behind GMM</h3>
                    <p>A Gaussian Mixture Model is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions with unknown parameters. Mathematically, GMM represents the probability density of the data as a weighted sum of K Gaussian components, where each component has its own mean vector (Œº<sub>k</sub>) and covariance matrix (Œ£<sub>k</sub>). The model is defined by these parameters plus the mixture weights (œÄ<sub>k</sub>) that determine the relative contribution of each component. GMMs are typically trained using the Expectation-Maximization (EM) algorithm, which iteratively estimates the model parameters by alternating between assigning data points to components (E-step) and updating the parameters based on these assignments (M-step). This approach guarantees convergence to a local optimum of the likelihood function, making GMM a theoretically sound method for modeling complex data distributions.</p>
                    
                    <h3>GMM vs. K-means: A Deeper Comparison</h3>
                    <p>While both K-means and GMM are clustering algorithms, they differ fundamentally in their approach. K-means can be viewed as a simplified special case of GMM with equal, spherical covariance matrices and hard assignments. The key theoretical difference is that K-means minimizes within-cluster sum-of-squares distances, while GMM maximizes the likelihood of the data under a generative model. This difference has significant practical implications: K-means draws straight-line boundaries between clusters, which works well for spherical, well-separated clusters but fails for complex shapes. GMM, in contrast, learns the full covariance structure of each cluster, allowing it to capture elliptical shapes, varying cluster sizes, and different orientations. Additionally, GMM's probabilistic nature enables it to handle overlapping clusters gracefully by assigning probabilities instead of making hard decisions. This makes GMM more flexible but also more computationally intensive and potentially prone to overfitting with limited data. The choice between these algorithms ultimately depends on the specific characteristics of your data and the goals of your analysis.</p>
                </div>
                <div class="col-md-4">
                    <div class="card">
                        <div class="card-body">
                            <h5 class="card-title">Key Concept</h5>
                            <p>A Gaussian Mixture Model represents a distribution as a weighted sum of Gaussian distributions.</p>
                            <div id="gmm-intro-formula" class="formula-display"></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="algorithm-walkthrough" class="my-5">
            <h2>üî¨ Step-by-Step: How GMM Works</h2>
            <p class="lead">Let's dive deep into the inner workings of the Gaussian Mixture Model algorithm by walking through each step of the process.</p>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>The Algorithm Behind GMM</h3>
                    <p>The Gaussian Mixture Model uses the Expectation-Maximization (EM) algorithm to find the maximum likelihood parameters of the mixture model. This iterative algorithm allows us to find the parameters of a statistical model where there are unobserved latent variables - in this case, the cluster assignments.</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>1. Model Definition</h3>
                    <p>A Gaussian Mixture Model represents a probability distribution as a weighted sum of K Gaussian components:</p>
                    
                    <div class="card mb-4">
                        <div class="card-body">
                            <div class="formula-display" id="model-definition-formula">
                                p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
                            </div>
                        </div>
                    </div>
                    
                    <p>Where:</p>
                    <ul>
                        <li><strong>p(x)</strong> is the probability density at point x</li>
                        <li><strong>K</strong> is the number of mixture components (clusters)</li>
                        <li><strong>œÄ<sub>k</sub></strong> are the mixture weights (must sum to 1)</li>
                        <li><strong>N(x|Œº<sub>k</sub>, Œ£<sub>k</sub>)</strong> is the probability density of the multivariate Gaussian distribution with mean Œº<sub>k</sub> and covariance matrix Œ£<sub>k</sub></li>
                    </ul>
                    
                    <p>The model parameters we need to estimate are:</p>
                    <ul>
                        <li>Mixture weights: œÄ<sub>1</sub>, œÄ<sub>2</sub>, ..., œÄ<sub>K</sub></li>
                        <li>Means: Œº<sub>1</sub>, Œº<sub>2</sub>, ..., Œº<sub>K</sub></li>
                        <li>Covariance matrices: Œ£<sub>1</sub>, Œ£<sub>2</sub>, ..., Œ£<sub>K</sub></li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <h3>2. Latent Variables and Log-Likelihood</h3>
                    <p>For each data point x<sub>i</sub>, we introduce a latent variable z<sub>i</sub> representing which component generated that point. We can think of z<sub>i</sub> as a K-dimensional one-hot vector where z<sub>ik</sub> = 1 if point i belongs to component k, and 0 otherwise.</p>
                    
                    <p>The complete-data log-likelihood (if we knew the latent variables) would be:</p>
                    
                    <div class="card mb-4">
                        <div class="card-body">
                            <div class="formula-display" id="complete-likelihood-formula">
                                \ln p(\mathbf{X}, \mathbf{Z}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{i=1}^{N} \sum_{k=1}^{K} z_{ik} \ln(\pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k))
                            </div>
                        </div>
                    </div>
                    
                    <p>However, we don't know the true assignments, so we use the EM algorithm to iteratively optimize this likelihood.</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <h3>3. Initialization</h3>
                    <p>We begin by initializing the parameters of our model. Proper initialization is crucial as the EM algorithm can converge to local optima. Common initialization strategies include:</p>
                    <ul>
                        <li><strong>Random initialization:</strong> Randomly select K data points as initial means, initialize covariances as identity matrices, and set weights to 1/K.</li>
                        <li><strong>K-means initialization:</strong> Run K-means clustering first and use the resulting centroids as initial means.</li>
                        <li><strong>Multiple restarts:</strong> Run the algorithm multiple times with different initializations and select the best result.</li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <div class="plot-container" id="em-process-plot"></div>
                    <p class="text-center">Evolution of GMM parameters over iterations</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-12">
                    <h3>4. Expectation-Maximization (EM) Algorithm</h3>
                    <p>The EM algorithm alternates between two steps:</p>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <div class="card mb-4">
                        <div class="card-body">
                            <h4>E-Step (Expectation)</h4>
                            <p>In the E-step, we estimate the expected values of the latent variables given our current model parameters. This results in computing "responsibilities" - the posterior probability that component k generated data point i:</p>
                            <div class="formula-display" id="estep-formula">
                                \gamma(z_{ik}) = \frac{\pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
                            </div>
                            <p>These responsibilities replace the unknown binary indicators z<sub>ik</sub> with their expected values. Each data point now has a probability distribution over all K clusters instead of a single cluster assignment.</p>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card mb-4">
                        <div class="card-body">
                            <h4>M-Step (Maximization)</h4>
                            <p>In the M-step, we update our model parameters to maximize the expected complete-data log-likelihood, using the responsibilities from the E-step:</p>
                            <div class="formula-display" id="mstep-mu-formula">
                                \boldsymbol{\mu}_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) \mathbf{x}_i}{\sum_{i=1}^{N} \gamma(z_{ik})}
                            </div>
                            <div class="formula-display" id="mstep-sigma-formula">
                                \boldsymbol{\Sigma}_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) (\mathbf{x}_i - \boldsymbol{\mu}_k^{new})(\mathbf{x}_i - \boldsymbol{\mu}_k^{new})^T}{\sum_{i=1}^{N} \gamma(z_{ik})}
                            </div>
                            <div class="formula-display" id="mstep-pi-formula">
                                \pi_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik})}{N}
                            </div>
                            <p>We can interpret N<sub>k</sub> = Œ£<sub>i</sub> Œ≥(z<sub>ik</sub>) as the effective number of points assigned to cluster k.</p>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-md-7">
                    <h3>5. Convergence</h3>
                    <p>After each E-step and M-step iteration, we repeat the process until the algorithm converges. Convergence can be determined by:</p>
                    <ul>
                        <li>Monitoring the change in log-likelihood between iterations</li>
                        <li>Checking if the parameter updates fall below some threshold</li>
                        <li>Reaching a maximum number of iterations</li>
                    </ul>
                    <p>The log-likelihood is guaranteed to increase (or remain the same) with each iteration, ensuring eventual convergence. However, it may converge to a local maximum rather than the global maximum.</p>
                    <p>The log-likelihood can be computed as:</p>
                    <div class="formula-display" id="log-likelihood-formula">
                        \ln p(\mathbf{X}|\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \sum_{i=1}^{N} \ln \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)
                    </div>
                    
                    <p>Once the algorithm converges, we have our final model parameters - the mixture weights œÄ<sub>k</sub>, mean vectors Œº<sub>k</sub>, and covariance matrices Œ£<sub>k</sub> for each of the K components. These parameters completely define our Gaussian Mixture Model and can be used for clustering, density estimation, or generating new samples from the learned distribution.</p>
                </div>
                <div class="col-md-5">
                    <div class="plot-container" id="likelihood-plot"></div>
                    <p class="text-center">Log-likelihood increases with each iteration until convergence</p>
                </div>
            </div>
        </section>
        
        <section id="interactive-gmm-example" class="my-5">
            <h2>üß© Interactive GMM Example</h2>
            <p class="lead">Watch the EM algorithm in action with this interactive demonstration. See how GMM iteratively improves cluster assignments and parameters.</p>
            
            <div class="row mb-4">
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-body">
                            <h3>Synthetic Data</h3>
                            <p>Generate synthetic data to see how GMM fits to various cluster shapes and distributions.</p>
                            <div id="interactive-data-plot" class="plot-container" style="height: 300px; min-height: 300px; width: 100%; position: relative;"></div>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card">
                        <div class="card-body">
                            <h3>EM Algorithm Steps</h3>
                            <p>Watch the algorithm iterate through E-steps and M-steps:</p>
                            <div id="interactive-em-plot" class="plot-container" style="height: 300px; min-height: 300px; width: 100%; position: relative;"></div>
                            <div class="d-flex justify-content-center mt-3">
                                <button id="em-iterate-btn" class="btn btn-primary mx-2">Next Iteration</button>
                                <button id="em-reset-btn" class="btn btn-secondary mx-2">Reset</button>
                                <button id="new-data-btn" class="btn btn-success mx-2">New Data</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="row">
                <div class="col-md-8">
                    <div class="card">
                        <div class="card-body">
                            <h3>Iteration Details</h3>
                            <div class="row">
                                <div class="col-md-6">
                                    <h4>Current Parameter Values:</h4>
                                    <div id="parameter-details" class="code-block p-3 bg-dark text-light">
                                        <p><strong>Iteration:</strong> <span id="current-iteration">0</span></p>
                                        <p><strong>Log-Likelihood:</strong> <span id="current-likelihood">-</span></p>
                                        <hr>
                                        <p><strong>Mixture Weights (œÄ):</strong></p>
                                        <div id="mixture-weights"></div>
                                        <hr>
                                        <p><strong>Mean Vectors (Œº):</strong></p>
                                        <div id="mean-vectors"></div>
                                    </div>
                                </div>
                                <div class="col-md-6">
                                    <h4>Responsibilities:</h4>
                                    <p>Each point has a probability of belonging to each cluster.</p>
                                    <div id="interactive-responsibilities-plot" class="plot-container" style="height: 300px; min-height: 300px; width: 100%; position: relative;"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="col-md-4">
                    <div class="card">
                        <div class="card-body">
                            <h3>Convergence</h3>
                            <div id="interactive-convergence-plot" class="plot-container" style="height: 300px; min-height: 300px; width: 100%; position: relative;"></div>
                            <p>The log-likelihood increases with each iteration until convergence.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section id="kmeans-to-gmm" class="my-5">
            <h2>üîç From K-Means to GMM</h2>
            <div class="row">
                <div class="col-md-6">
                    <h3>K-Means vs GMM Comparison</h3>
                    <table class="table table-bordered">
                        <thead>
                            <tr>
                                <th>Feature</th>
                                <th>K-Means</th>
                                <th>GMM</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Assignment</td>
                                <td>Hard (binary)</td>
                                <td>Soft (probabilistic)</td>
                            </tr>
                            <tr>
                                <td>Cluster shape</td>
                                <td>Spherical</td>
                                <td>Elliptical</td>
                            </tr>
                            <tr>
                                <td>Distance measure</td>
                                <td>Euclidean</td>
                                <td>Mahalanobis</td>
                            </tr>
                            <tr>
                                <td>Model complexity</td>
                                <td>Low</td>
                                <td>Higher</td>
                            </tr>
                            <tr>
                                <td>Training method</td>
                                <td>Iterative optimization</td>
                                <td>Expectation-Maximization (EM)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="col-md-6">
                    <h3>Visual Comparison</h3>
                    <div class="row">
                        <div class="col-md-6">
                            <div id="kmeans-plot" class="plot-container"></div>
                            <p class="text-center">K-Means clustering result</p>
                        </div>
                        <div class="col-md-6">
                            <div id="gmm-plot" class="plot-container"></div>
                            <p class="text-center">GMM clustering result</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </div>

    <footer class="bg-dark text-light py-3 mt-5">
        <div class="container text-center">
            <p>¬© 2023 GMM Tutorial | Created for educational purposes</p>
        </div>
    </footer>

    <!-- JavaScript dependencies -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.8.2/dist/d3.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/plotly.js@2.18.2/dist/plotly.min.js" defer></script>
    <script src="js/script.js" defer></script>
</body>
</html> 